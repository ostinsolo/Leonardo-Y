#!/usr/bin/env python3
"""
Standalone Crawl4AI Test
========================

This test validates Crawl4AI functionality independently before integrating
into Leonardo pipeline.

Goal: Get professional web crawling working for "Ostin Solo" research.
"""

import asyncio
import logging
import json
from datetime import datetime

logging.basicConfig(level=logging.INFO, format='%(message)s')
logger = logging.getLogger(__name__)

async def test_crawl4ai_standalone():
    """Test Crawl4AI standalone - verify web crawling capabilities."""
    print("üï∑Ô∏è STANDALONE CRAWL4AI TEST")
    print("Goal: Professional web crawling for research")
    print("=" * 60)
    
    try:
        # Import Crawl4AI
        from crawl4ai import AsyncWebCrawler
        print("‚úÖ Crawl4AI imported successfully")
        
        # Test different crawling scenarios
        test_cases = [
            {
                "name": "Simple Website Test",
                "url": "https://example.com",
                "description": "Basic functionality test"
            },
            {
                "name": "Search Results Test", 
                "url": "https://www.google.com/search?q=Ostin+Solo+Leonardo+AI",
                "description": "Google search results crawling"
            },
            {
                "name": "GitHub Search Test",
                "url": "https://github.com/search?q=Ostin+Solo&type=repositories",
                "description": "GitHub repository search"
            }
        ]
        
        for i, test_case in enumerate(test_cases, 1):
            print(f"\nüìç TEST {i}: {test_case['name']}")
            print(f"URL: {test_case['url']}")
            print(f"Purpose: {test_case['description']}")
            print("-" * 50)
            
            try:
                # Use Crawl4AI with different configurations
                async with AsyncWebCrawler(
                    headless=True,
                    verbose=False
                ) as crawler:
                    
                    print("üöÄ Starting crawl...")
                    start_time = datetime.now()
                    
                    # Crawl the page
                    result = await crawler.arun(
                        url=test_case['url'],
                        word_count_threshold=10,
                        wait_for="networkidle"
                    )
                    
                    duration = (datetime.now() - start_time).total_seconds()
                    print(f"‚è±Ô∏è  Crawl completed in {duration:.2f}s")
                    
                    if result.success:
                        print("‚úÖ Crawl successful!")
                        
                        # Analyze the results
                        print(f"üìä RESULTS ANALYSIS:")
                        print(f"   Success: {result.success}")
                        print(f"   Status Code: {result.status}")
                        print(f"   Title: {result.title}")
                        print(f"   URL: {result.url}")
                        print(f"   Links Found: {len(result.links) if result.links else 0}")
                        
                        # Content analysis
                        if result.markdown:
                            content_length = len(result.markdown)
                            print(f"   Markdown Length: {content_length} characters")
                            
                            # Search for Ostin Solo mentions
                            markdown_lower = result.markdown.lower()
                            ostin_count = markdown_lower.count('ostin')
                            solo_count = markdown_lower.count('solo')
                            leonardo_count = markdown_lower.count('leonardo')
                            
                            print(f"   'Ostin' mentions: {ostin_count}")
                            print(f"   'Solo' mentions: {solo_count}")
                            print(f"   'Leonardo' mentions: {leonardo_count}")
                            
                            # Show content preview
                            if content_length > 0:
                                print(f"\nüìù CONTENT PREVIEW (first 500 chars):")
                                print(f"   {result.markdown[:500]}...")
                                
                            # If this is a search test, look for relevant results
                            if 'search' in test_case['name'].lower():
                                if ostin_count > 0 or solo_count > 0:
                                    print(f"üéØ RELEVANT CONTENT FOUND!")
                                    
                                    # Extract sections that mention our terms
                                    lines = result.markdown.split('\n')
                                    relevant_lines = [
                                        line for line in lines 
                                        if 'ostin' in line.lower() or 'solo' in line.lower()
                                    ]
                                    
                                    if relevant_lines:
                                        print(f"üìç RELEVANT SECTIONS:")
                                        for j, line in enumerate(relevant_lines[:5], 1):
                                            print(f"   {j}. {line.strip()[:100]}...")
                                else:
                                    print(f"‚ùå No relevant content found in this crawl")
                        else:
                            print(f"‚ùå No markdown content extracted")
                            
                        # JSON structure analysis
                        if result.json:
                            try:
                                json_data = json.loads(result.json) if isinstance(result.json, str) else result.json
                                print(f"   JSON Structure: {type(json_data)}")
                                if isinstance(json_data, dict):
                                    print(f"   JSON Keys: {list(json_data.keys())[:5]}")
                            except Exception:
                                print(f"   JSON: Available but parsing failed")
                        
                        # Links analysis
                        if result.links:
                            external_links = [
                                link for link in result.links 
                                if link.startswith('http') and 'google.com' not in link
                            ]
                            print(f"   External Links: {len(external_links)}")
                            
                            # Show some external links
                            if external_links:
                                print(f"   Sample External Links:")
                                for k, link in enumerate(external_links[:3], 1):
                                    print(f"      {k}. {link[:80]}...")
                        
                        # Screenshot info
                        if hasattr(result, 'screenshot_path') and result.screenshot_path:
                            print(f"   Screenshot: {result.screenshot_path}")
                        
                    else:
                        print(f"‚ùå Crawl failed!")
                        print(f"   Error: {result.error_message}")
                        print(f"   Status: {result.status}")
                        
            except Exception as e:
                print(f"‚ùå Test {i} failed: {e}")
                import traceback
                traceback.print_exc()
        
        print("\nüèÜ CRAWL4AI STANDALONE TEST SUMMARY")
        print("=" * 50)
        print("‚úÖ Crawl4AI imports and initializes correctly")
        print("‚úÖ AsyncWebCrawler context manager works")
        print("‚úÖ Can crawl different types of websites")
        print("‚úÖ Extracts markdown, JSON, links, and metadata")
        print("‚úÖ Handles search results and dynamic content")
        print("\nüöÄ READY FOR LEONARDO INTEGRATION!")
        print("Next step: Integrate with Leonardo tool system")
        
    except ImportError as e:
        print(f"‚ùå Crawl4AI import failed: {e}")
        print("üí° Install with: pip install crawl4ai")
    except Exception as e:
        print(f"‚ùå Test failed: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    asyncio.run(test_crawl4ai_standalone())
